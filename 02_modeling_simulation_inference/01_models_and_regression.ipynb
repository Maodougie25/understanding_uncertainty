{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a26b3950",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# From Likelihood to Models\n",
    "### Understanding Uncertainty"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d27b134",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Introduction\n",
    "- In the last notebook, we introduced maximum likelihood with one or two parameters: Basically, just the random variable and its mean and perhaps variance\n",
    "- This was a great tool, but not quite what we want for machine learning and data science, where there are typically many observations and many variables\n",
    "- We want to introduce context into the discussion: features, covariates, controls, etc.\n",
    "- We are not focusing on machine learning best practices today (train-test split, cross validation, feature engineering, etc.). We're just fitting models and simulating outcomes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e4de5af",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Introduction\n",
    "- We're going to use StatsModels to do our maximum likelihood estimation, because it gives us parameter estimates and point estimates quickly; I'm not super fond of StatsModels, but it gets the job done in Python\n",
    "- The models we look at (linear regression, logistic regression, poisson regression, proportional hazards*) are extremely useful workhorse models, but we're interested in them from a probabilistic perspective: How are we modeling uncertainty, and incorporating covariates into our understanding of prediction?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f1fcbbf",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Basic Approach\n",
    "- We have our densities/distributions that yield log-likelihoods, modeling noise/shock/uncertainty/measurement error/etc.\n",
    "- Those densities/distributions are **parameterized** by variables like $\\mu$, $\\sigma$, $\\lambda$, etc.\n",
    "- We want to use our data, we want to use our theories, we want to build and control our predictions deliberately\n",
    "- So \n",
    "    1. Pick a parameterized density/distribution with support matching your observed outcomes (e.g. any number, positive number, binary, count, duration, etc.)\n",
    "    2. Replace the parameters $\\mu$, $\\sigma$, $\\lambda$ by functions of your data. These will often be roughly linear, like $\\mu = x_i \\beta$ or $\\lambda = e^{x_i \\beta}$, but your $x_i$'s will typically result from feature engineering. \n",
    "    3. Pick the model coefficients/weights ($\\beta$) to maximize the likelihood/minimize the loss\n",
    "    4. Use your model to make predictions (prediction: $\\hat{y}$), quantify prediction uncertainty (simulation: $\\hat{y} + \\sigma \\varepsilon$), and quantify model uncertainy (inference: bootstrap the sampling distribution of $\\beta$ or $\\hat{y}$ or anything else of interest)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e167fe9c",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Roadmap\n",
    "1. Linear and Log-Linear Regression (Predict $ -\\infty < \\hat{y} < \\infty $ or $\\hat{y} >0$)\n",
    "2. Logistic Regression (Predict $ 0 < \\hat{y} <1 $)\n",
    "3. Poisson Regression (Predict a non-negative count $\\hat{k} \\in \\{ 0, 1, 2, ... \\}$)\n",
    "\n",
    "All of these frameworks are just an application of MLE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07660d5d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 1. Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ddf185",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Linear Regression as MLE\n",
    "- The workhorse predictive model for everyone who works with data is linear regression: $y_i = x_i \\cdot \\beta + \\sigma \\varepsilon_i$, where $\\varepsilon_i$ is a random variable with the standard normal distribution\n",
    "- There are some good reasons why this model is so popular:\n",
    "    1. It has a closed form solution: $\\hat{\\beta} = (X^{\\top}X)^{-1} X^{\\top}y$\n",
    "    2. The coefficients are reasonably easy to interpret: A $1$% change in $x_{ik}$ leads to a $\\beta_k$% change in $y_i$\n",
    "    3. Even if the true model is non-linear, we can include transformations of the features/control variables to match almost any continuous pattern in the data\n",
    "    4. If the model is unidentified/over-parameterized, we can use LASSO or Ridge regression to regularize and do model selection\n",
    "    5. We have hardware that computes inner products like $x_i \\cdot \\beta$ very efficiently"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d714cc1c",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Case: Hedonic Pricing\n",
    "- Using the Ames Price data, predict (log) house prices as a function of amenities/features of the property."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b4a981f",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Deriving the Likelihood\n",
    "- If we assume that $y_i = x_i \\cdot \\beta + \\sigma \\varepsilon_i$, then we can specify the error term as:\n",
    "$$\n",
    "\\varepsilon_i = \\frac{y_i - x_i \\cdot \\beta}{\\sigma}\n",
    "$$\n",
    "- If we assume that the error term has the standard normal distribution, $\\varepsilon \\sim N(0,1)$, then observation $i$'s contribution to the likelihood is:\n",
    "$$\n",
    "\\dfrac{1}{\\sigma} \\phi \\left( \\frac{y_i - x_i \\cdot \\beta}{\\sigma} \\right) = \\dfrac{1}{\\sigma \\sqrt{2\\pi} } \\exp \\left( - \\frac{1}{2} \\left(  \\frac{y_i - x_i \\cdot \\beta}{\\sigma}\\right)^2 \\right) \n",
    "$$\n",
    "- Multiplying all the contributions together yields:\n",
    "$$\n",
    "\\begin{alignat*}{2}\n",
    "L(\\beta,\\sigma) &=& \\prod_{i=1}^n \\dfrac{1}{\\sigma} \\phi \\left( \\frac{y_i - x_i \\cdot \\beta}{\\sigma} \\right) \\\\\n",
    "&=& \\prod_{i=1}^n \\dfrac{1}{\\sqrt{2\\pi} \\sigma} \\exp \\left( - \\frac{1}{2} \\left(  \\frac{y_i - x_i \\cdot \\beta}{\\sigma}\\right)^2 \\right) \n",
    "\\end{alignat*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5e76132",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## The Normal-Linear Log-Likelihood\n",
    "- And taking logs and simplifying (algebra in the last notebook) gives us:\n",
    "$$\n",
    "\\ell(\\beta, \\sigma) = -n \\log(\\sqrt{2\\pi}) - n \\log(\\sigma) - \\sum_{i=1}^n \\frac{1}{2} \\left( \\frac{y_i - x_i \\cdot \\beta}{\\sigma} \\right)^2\n",
    "$$\n",
    "- This is the same as replacing $\\mu = x_i \\cdot \\beta$ in the simpler log-likelihood we looked at earlier: We're modeling the expected outcome, given $x_i$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cdcfbe2",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## And Notice:\n",
    "- We can re-arrange a bit as:\n",
    "$$\n",
    "\\ell(\\beta, \\sigma) = -n \\log(\\sqrt{2\\pi}) - n \\log(\\sigma) - \\dfrac{1}{2 \\sigma^2} \\underbrace{ \\sum_{i=1}^n \\left( y_i - x_i \\cdot \\beta \\right)^2 }_{\\text{Sum of Squared Error}}\n",
    "$$\n",
    "- In some sense, this approach allows us to break the problem into solving for $\\beta$ to get our **point estimates** $\\hat{y}_i = x_i \\cdot \\hat{\\beta}$, and then model the distribution around those point estimates\n",
    "- Whenever you are minimizing $SSE$, you are essentially assuming normally distributed errors, whether you make it explicit or not\n",
    "- You could make the distributions of the errors more interesting: For example, you could also parameterize and model the variance of the errors, using something like $\\sigma_i = e^{x_i \\cdot \\gamma}$; this is called **heteroskedasticity**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20dcceb8",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Exercise: Predictive Densities\n",
    "Notice that once we have a fitted model, $(\\hat{\\beta}, \\hat{\\sigma})$, we can make predictions: \n",
    "$$\n",
    "\\hat{y}_i \\sim \\text{Normal}( \\text{mean} = x_i \\cdot \\hat{\\beta}, \\text{variance} = \\hat{\\sigma}^2)\n",
    "$$\n",
    "\n",
    "- Make a plot showing the density of the house price prediction, for every house in the dataset.\n",
    "- For each house $i$, draw 1000 standard normal shocks, and compute a sample of realized values $\\hat{y}_i = x_i \\cdot \\hat{\\beta} + \\hat{\\sigma} \\varepsilon_i$. Plot the kernel densities for each property, and compare them with the theoretical distribution. (Hint: From SciPy, import norm, and use norm.rvs(size=1000, random_state=100) to generate your shocks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b94f207",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Case:\n",
    "- Now, let's predict who survives in the breast cancer data set as a function of characteristics, and their distribution of outcomes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51a3b88f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 2. Logistic Regression as MLE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e953fa5",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Predicting Probabilities\n",
    "- OLS/Linear regression is great, powerful, simple; not the end of the story, but it's hard to understate how useful it can be\n",
    "- But it places no restrictions on the values it can predict\n",
    "- Often we need real probabilities: Values between 0 and 1 that represent the likelihood something happens\n",
    "- OLS can yield probabilities above 1 and below 0. \n",
    "- What's a good alternative?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ce1553b",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Cases: \n",
    "- Using the NHANES data, predict who has insurance based on demographic characteristics.\n",
    "- Using the METABRIC data, predict who survives as a function of characteristics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eb665b4",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Bernoulli Likelihood/Binary Cross Entropy\n",
    "- We saw that the log-likelihood for a Bernoulli random variable is\n",
    "$$\n",
    "\\ell(p) = \\sum_{i=1}^N y_i \\log(p) + (1-y_i) \\log(1-p)\n",
    "$$\n",
    "with MLE $\\hat{p} = \\bar{y}$.\n",
    "- Logistic regression is built on the Bernoulli distribution, replacing \n",
    "$$\n",
    "p_i = F(x_i \\cdot \\beta) = \\dfrac{1}{1+e^{-x_i \\cdot \\beta}}\n",
    "$$\n",
    "- So we're using the covariates/controls/features $x_i$ to model the probability that $y_i =1$ for each $i$\n",
    "- This is the simplest neural network (one input layer, zero hidden layers, two nodes in the output layer (0,1)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1432ac1a",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## The Logistic Log-Likelihood\n",
    "- We then substitute this into the Bernoulli likelihood\n",
    "$$\n",
    "\\sum_{i=1}^n y_i \\log(p_i) + (1-y_i) \\log(1-p_i)\n",
    "$$\n",
    "to get\n",
    "\\begin{alignat*}{2}\n",
    "\\ell(\\beta) &=& \\sum_{i=1}^n y_i \\log \\left( \\dfrac{1}{1+e^{-x_i \\cdot \\beta}} \\right) + (1-y_i) \\log \\left( \\dfrac{e^{-x_i \\cdot \\beta}}{1+e^{-x_i \\cdot \\beta}}\\right) \\\\\n",
    "&=& \\sum_{i=1}^n y_i \\log \\left( \\dfrac{e^{x_i \\cdot \\beta}}{1+e^{x_i \\cdot \\beta}} \\right) + (1-y_i) \\log \\left( \\dfrac{1}{1+e^{x_i \\cdot \\beta}}\\right)\n",
    "\\end{alignat*}\n",
    "- This looks complex, but we've seen how, for logistic regression, $f(z)=F(z)(1-F(z))$, which dramatically simplifies calculations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a09d9e74",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Exercise: Interpreting Non-Linear Effects\n",
    "- For linear regression, a \"small\" change in variable $x_{ik}$ resulted in a $\\beta_k$ change in the prediction $\\hat{y}_i$ for every $i$, regardless of the values of the variables: Interpreting how changes in features changed predictions was straightforward, because for a linear model, $\\partial \\hat{y}/\\partial x_{ik} = \\beta_k$\n",
    "- For logistic regression, this interpretation is incorrect, since\n",
    "$$\n",
    "p_i = \\frac{1}{1+e^{- x_i \\cdot \\beta}},\n",
    "$$\n",
    "and small changes in $x_{ik}$ do not lead to $\\beta_k$ changes in $p_i$\n",
    "- What's the derivative of $p_i$ with respect to $x_{ik}$?\n",
    "$$\n",
    "\\frac{\\partial p_i}{\\partial x_{ik}} = \\beta_k \\dfrac{1}{1+e^{- x_i \\cdot \\beta}} \\dfrac{e^{- x_i \\cdot \\beta}}{1+e^{- x_i \\cdot \\beta}} = \\beta_k p_i (1-p_i)\n",
    "$$\n",
    "- So, on average across the sample, how does a change in variable $k$ impact the predictions? The **average marginal effect (AME)** is\n",
    "$$\n",
    "AME_k =  \\dfrac{1}{n} \\sum_{i=1}^n \\beta_k p_i (1-p_i)\n",
    "$$\n",
    "- A popular alternative is to compute the probability at the average for each of the variables,\n",
    "$$\n",
    "\\bar{p} = \\frac{1}{1+e^{- \\bar{x} \\cdot \\beta}},\n",
    "$$\n",
    "and then compute the **marginal effect at the mean**:\n",
    "$$\n",
    "MEM_k = \\beta_k \\bar{p} (1-\\bar{p})\n",
    "$$\n",
    "- For the METABRIC survival example, use logistic regression and OLS to predict surival. For the logistic regression model, compute the AME and MEM for your model, and compare the results to the OLS coefficients."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4709edee",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 3. Poisson Regression as MLE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16380c9b",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Count Data\n",
    "- One of the motivations for logistic regression was that linear regression might deliver values outside $[0,1]$; i.e. not probabilities\n",
    "- A similar problem comes up with data that have discrete values, like $\\{0, 1, 2, ... \\}$; we call this **count data**\n",
    "- If you are talking about millions of pounds of fish, you can fudge the discreteness of the data, but if you are talking about the number of cancerous lymph nodes detected, you can't\n",
    "- How do we repurpose MLE for estimating discrete quantities?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa985054",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Case Study: \n",
    "- Build a model to predict 'Lymph nodes examined positive' and 'Tumor Size' using the METABRIC data, and a distribution of patient outcomes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a9b69d8",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Poisson Regression as MLE\n",
    "- Which distributions do we have that are useful for count data? The Poisson is the easiest to work with (but the Negative Binomial is another popular one)\n",
    "- The Poisson mass function is\n",
    "$$\n",
    "pr[y_i = k] = \\dfrac{\\lambda^k e^{-\\lambda}}{k!}\n",
    "$$\n",
    "where $\\lambda >0$, for $k = 0, 1, ...$\n",
    "- This is for **count data**: Goals scored in a match, number of infected lymph nodes, etc.\n",
    "- We are fairly forgiving of using OLS with count data, but when it matters, we want to switch to a method like Poisson or Negative Binomial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32dbd987",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Parameterizing $\\lambda$\n",
    "- We want to predict the count $y_i$, given our covariates $x_i$\n",
    "- The natural parameter to replace is $\\lambda$, but it has to be non-negative, or the model blows up\n",
    "- The standard way to handle this problem is to pick\n",
    "$$\n",
    "\\lambda_i = e^{x_i \\beta}\n",
    "$$\n",
    "so that regardless of the $\\beta$ we pick, we'll always get a positive $\\lambda$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "352877ab",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Deriving the Log-Likelihood\n",
    "- Let's build the log-likelihood for this case\n",
    "- The probability that $y_i = k$ is\n",
    "$$\n",
    "pr[y_i = k] = \\dfrac{(e^{x_i \\beta})^k e^{-e^{x_i \\beta}} }{k!}\n",
    "$$\n",
    "- The log is\n",
    "$$\n",
    "\\log(pr[y_i=k]) = k \\times x_i \\cdot \\beta - e^{-x_i \\beta} - \\log(k!)\n",
    "$$\n",
    "- And substituting $y_i$ for $k$ yields the contribution of each observation $i$ to the log-likelihood:\n",
    "$$\n",
    "y_i \\times x_i \\cdot \\beta - e^{-x_i \\beta} - \\log(y_i!)\n",
    "$$\n",
    "- Summing yields the log-probability of observing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d58a31a",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## The Poisson Log-Likelihood\n",
    "\n",
    "- Then our log-likelihood is\n",
    "$$\n",
    "\\ell(\\beta) = \\sum_{i=1}^n [ y_i \\times x_i \\cdot \\beta - e^{-x_i \\beta} - \\log(y_i!)\n",
    "]\n",
    "$$\n",
    "- Again, we don't solve by hand: We take the gradient with respect to $\\beta$, and let the computer do gradient descent to get our $\\hat{\\beta}$ estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "076e3e43",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Exercise:\n",
    "- For the METABRIC data, build a model and predict 'Lymph nodes examined positive' for each observation \n",
    "- Bootstrap the parameters of your model and plot their sampling densities/distributions\n",
    "- Bootstrap the densities of your predictions for each observation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "866d33bf",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Conclusion\n",
    "- These are all great entry-level options for modeling and prediction \n",
    "    - Linear/log-linear: Real or non-negative reponse values\n",
    "    - Logistic: Probabilities\n",
    "    - Poisson: Counts\n",
    "    - (Proportional Hazards: Predicting duration/arrival/surival times)\n",
    "- When we say \"learning\" in machine learning, we literally mean: The model parameters are being updated by way of gradient descent to better fit the data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
